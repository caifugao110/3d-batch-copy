import os
import shutil
import csv
import time
import configparser
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

def load_configuration():
    """
    åŠ è½½é…ç½®æ–‡ä»¶å¹¶è§£æè®¾ç½®
    
    Returns:
        config: é…ç½®å¯¹è±¡
        source_dirs: å®Œæ•´çš„æºç›®å½•è·¯å¾„åˆ—è¡¨
        target_dir: ç›®æ ‡ç›®å½•å®Œæ•´è·¯å¾„
        list_file: åŸå§‹æ–‡ä»¶åˆ—è¡¨å®Œæ•´è·¯å¾„
        log_file: æ—¥å¿—æ–‡ä»¶å®Œæ•´è·¯å¾„
        max_workers: æœ€å¤§çº¿ç¨‹æ•°
        retry_attempts: é‡è¯•æ¬¡æ•°
    """
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    config_path = os.path.join(current_dir, "config.ini")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(config_path):
        print(f"ğŸ”¥ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("è¯·ç¡®ä¿ config.ini æ–‡ä»¶ä¸è„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹")
        exit(1)
    
    try:
        # åˆ›å»ºé…ç½®è§£æå™¨
        config = configparser.ConfigParser()
        # ä¿ç•™é€‰é¡¹çš„å¤§å°å†™
        config.optionxform = lambda option: option
        config.read(config_path, encoding='utf-8')
        
        # è¯»å–åŸºæœ¬è·¯å¾„é…ç½®
        drive_letter = config.get("Paths", "drive_letter").strip()
        target_dir_name = config.get("Paths", "target_dir_name")
        original_list_file = config.get("Paths", "original_list_file")
        log_file_name = config.get("Paths", "log_file")
        
        # æ„å»ºæºç›®å½•å®Œæ•´è·¯å¾„åˆ—è¡¨
        source_dirs = []
        for key in config["SourceDirectories"]:
            relative_path = config.get("SourceDirectories", key)
            full_path = f"{drive_letter}:\\{relative_path}"
            source_dirs.append(full_path)
        
        # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
        target_dir = os.path.join(current_dir, target_dir_name)
        list_file = os.path.join(current_dir, original_list_file)
        log_file_path = os.path.join(current_dir, log_file_name)
        
        # è¯»å–æ€§èƒ½è®¾ç½®
        max_workers = config.getint("Settings", "max_workers", fallback=12)
        retry_attempts = config.getint("Settings", "retry_attempts", fallback=3)
        
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ:")
        print(f"   é©±åŠ¨å™¨ç›˜ç¬¦: {drive_letter}")
        print(f"   æºç›®å½•æ•°é‡: {len(source_dirs)}")
        print(f"   ç›®æ ‡ç›®å½•: {target_dir}")
        print(f"   æœ€å¤§çº¿ç¨‹æ•°: {max_workers}")
        print(f"   é‡è¯•æ¬¡æ•°: {retry_attempts}")
        
        return config, source_dirs, target_dir, list_file, log_file_path, max_workers, retry_attempts
        
    except Exception as e:
        print(f"ğŸ”¥ é…ç½®æ–‡ä»¶è§£æå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ config.ini æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        exit(1)

def clean_filename(name):
    """
    æ¸…ç†æ–‡ä»¶åï¼šå»é™¤ç‰¹å®šåç¼€å’Œæ ‡è¯†ç¬¦ï¼Œç»Ÿä¸€è½¬ä¸ºå°å†™
    
    Args:
        name: åŸå§‹æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    
    Returns:
        str: æ¸…ç†åçš„æ–‡ä»¶å
    """
    # å»é™¤æ–‡ä»¶åæœ«å°¾çš„ 'L'ï¼ˆå¦‚æœæœ‰ï¼‰
    if name.endswith("L"):
        name = name[:-1]
    
    # å¦‚æœæ–‡ä»¶åä¸­åŒ…å« 'L('ï¼Œåªä¿ç•™æ‹¬å·å‰çš„éƒ¨åˆ†
    if "L(" in name:
        parts = name.split("L(")
        name = parts[0]
    
    # è¿”å›å°å†™æ ¼å¼ä»¥è¿›è¡Œä¸åŒºåˆ†å¤§å°å†™çš„åŒ¹é…
    return name.lower()

def cleanup_target_directory(target_dir):
    """
    æ¸…ç†ç›®æ ‡ç›®å½•ä¸­çš„æ‰€æœ‰ .step æ–‡ä»¶
    
    Args:
        target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
    """
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç›®æ ‡ç›®å½•...")
    clean_count = 0
    
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(target_dir):
        print(f"ğŸ“ ç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º: {target_dir}")
        os.makedirs(target_dir, exist_ok=True)
        return
    
    # éå†ç›®æ ‡ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
    for file in os.listdir(target_dir):
        # åªå¤„ç† .step æ–‡ä»¶ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        if file.lower().endswith(".step"):
            try:
                file_path = os.path.join(target_dir, file)
                os.remove(file_path)
                clean_count += 1
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§æ–‡ä»¶å¤±è´¥: {file} - {e}")
    
    print(f"âœ… å·²æ¸…ç† {clean_count} ä¸ªæ—§æ–‡ä»¶")

def build_file_index(source_dirs):
    """
    æ„å»ºæ–‡ä»¶ç´¢å¼•ï¼ŒæŒ‰æ–‡ä»¶åå‰4ä¸ªå­—ç¬¦åˆ†ç»„ä»¥æé«˜æœç´¢æ•ˆç‡
    
    Args:
        source_dirs: æºç›®å½•è·¯å¾„åˆ—è¡¨
    
    Returns:
        defaultdict: æ–‡ä»¶ç´¢å¼•å­—å…¸ï¼Œæ ¼å¼ä¸º {prefix: [(clean_name, filename, directory), ...]}
    """
    print("â³ æ­£åœ¨æ„å»ºå…¨å±€æ–‡ä»¶ç´¢å¼•...")
    # ä½¿ç”¨ defaultdict è‡ªåŠ¨åˆå§‹åŒ–ç©ºåˆ—è¡¨
    index = defaultdict(list)
    start_time = time.time()
    total_files = 0
    
    # éå†æ‰€æœ‰æºç›®å½•
    for src_dir in source_dirs:
        try:
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”å¯è®¿é—®
            if not os.path.exists(src_dir):
                print(f"âš ï¸ è·¯å¾„ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®: {src_dir}")
                continue
            
            # ä½¿ç”¨ os.scandir() è¿›è¡Œé«˜æ•ˆçš„ç›®å½•éå†
            with os.scandir(src_dir) as entries:
                for entry in entries:
                    # åªå¤„ç† .step æ–‡ä»¶ä¸”å¿½ç•¥ç›®å½•
                    if entry.is_file() and entry.name.lower().endswith(".step"):
                        # åˆ†ç¦»æ–‡ä»¶åå’Œæ‰©å±•å
                        base_name = os.path.splitext(entry.name)[0]
                        # æ¸…ç†æ–‡ä»¶åç”¨äºåŒ¹é…
                        clean_base = clean_filename(base_name)
                        # ä½¿ç”¨å‰4ä¸ªå­—ç¬¦ä½œä¸ºç´¢å¼•é”®ï¼ˆå¦‚æœæ–‡ä»¶åè¶³å¤Ÿé•¿ï¼‰
                        prefix_key = clean_base[:4] if len(clean_base) >= 4 else clean_base
                        # å°†æ–‡ä»¶ä¿¡æ¯æ·»åŠ åˆ°ç´¢å¼•ä¸­
                        index[prefix_key].append((clean_base, entry.name, src_dir))
                        total_files += 1
                        
        except Exception as e:
            print(f"âš ï¸ ç›®å½•æ‰«æå¤±è´¥: {src_dir} - {e}")
    
    index_time = time.time() - start_time
    print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ: {len(index)} ä¸ªå‰ç¼€ç»„, {total_files} ä¸ªæ–‡ä»¶, è€—æ—¶ {index_time:.2f}ç§’")
    
    return index

def read_original_file_list(list_file):
    """
    è¯»å–åŸå§‹æ–‡ä»¶åˆ—è¡¨ CSV æ–‡ä»¶
    
    Args:
        list_file: åŸå§‹æ–‡ä»¶åˆ—è¡¨è·¯å¾„
    
    Returns:
        list: æ–‡ä»¶ååˆ—è¡¨
    """
    try:
        with open(list_file, "r", encoding="utf-8-sig") as f:
            reader = csv.reader(f)
            # è¯»å–ç¬¬ä¸€åˆ—çš„éç©ºè¡Œ
            all_lines = [row[0].strip() for row in reader if row and row[0].strip()]
        
        print(f"ğŸ“‹ å¾…å¤„ç†æ–‡ä»¶æ•°: {len(all_lines)}")
        return all_lines
        
    except Exception as e:
        print(f"ğŸ”¥ CSV æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        print(f"è¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®: {list_file}")
        exit(1)

def process_item(item, target_dir, index, retry_attempts):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„å¤åˆ¶ä»»åŠ¡
    
    Args:
        item: åŒ…å«åŸå§‹æ–‡ä»¶åå’Œæ¸…ç†åæœç´¢åçš„å…ƒç»„
        target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
        index: æ–‡ä»¶ç´¢å¼•å­—å…¸
        retry_attempts: é‡è¯•æ¬¡æ•°
    
    Returns:
        dict: å¤„ç†ç»“æœä¿¡æ¯
    """
    original_name, search_name = item
    # ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆç»Ÿä¸€ä½¿ç”¨ .STEP æ‰©å±•åï¼‰
    dst_file = os.path.join(target_dir, f"{original_name}.STEP")
    # ä½¿ç”¨å‰4ä¸ªå­—ç¬¦ä½œä¸ºæœç´¢é”®
    prefix_key = search_name[:4] if len(search_name) >= 4 else search_name

    # åœ¨å¯¹åº”å‰ç¼€ç»„ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
    if prefix_key in index:
        for clean_base, src_filename, src_dir in index[prefix_key]:
            # ä½¿ç”¨ startswith è¿›è¡Œå‰ç¼€åŒ¹é…
            if clean_base.startswith(search_name):
                # é‡è¯•æœºåˆ¶å¤„ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶é”å®šæˆ–ç½‘ç»œé—®é¢˜
                for attempt in range(retry_attempts):
                    try:
                        src_path = os.path.join(src_dir, src_filename)
                        # å¤åˆ¶æ–‡ä»¶å¹¶ä¿ç•™å…ƒæ•°æ®
                        shutil.copy2(src_path, dst_file)
                        return {
                            "status": "success",
                            "original": original_name,
                            "copied": src_filename,
                            "source": src_dir
                        }
                    except Exception as e:
                        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™ç­‰å¾…åé‡è¯•
                        if attempt < retry_attempts - 1:
                            # æŒ‡æ•°é€€é¿ç­–ç•¥ï¼šç­‰å¾…æ—¶é—´éšå°è¯•æ¬¡æ•°å¢åŠ 
                            time.sleep(2 ** attempt)
                        else:
                            # æœ€åä¸€æ¬¡å°è¯•ä»ç„¶å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                            return {
                                "status": "error",
                                "original": original_name,
                                "copied": f"å¤åˆ¶å¤±è´¥: {e}",
                                "source": src_dir
                            }
    
    # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶
    return {
        "status": "not_found",
        "original": original_name,
        "copied": "æœªæ‰¾åˆ°",
        "source": ""
    }

def write_result_log(log_file, result_log):
    """
    å°†å¤„ç†ç»“æœå†™å…¥æ—¥å¿—æ–‡ä»¶
    
    Args:
        log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        result_log: ç»“æœæ—¥å¿—åˆ—è¡¨
    """
    print("ğŸ“ æ­£åœ¨å†™å…¥æ—¥å¿—æ–‡ä»¶...")
    try:
        with open(log_file, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f)
            # å†™å…¥è¡¨å¤´
            writer.writerow(["åŸå§‹æ–‡ä»¶å", "å®é™…å¤åˆ¶æ–‡ä»¶å", "æ¥æºè·¯å¾„"])
            # å†™å…¥æ¯æ¡ç»“æœ
            for res in result_log:
                writer.writerow([res["original"], res["copied"], res["source"]])
        print(f"âœ… æ—¥å¿—å·²ä¿å­˜è‡³: {log_file}")
    except Exception as e:
        print(f"âš ï¸ æ—¥å¿—æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”§ 3D æ–‡ä»¶æ‰¹é‡å¤åˆ¶å·¥å…·")
    print("=" * 60)
    
    # è®°å½•ç¨‹åºå¼€å§‹æ—¶é—´
    program_start_time = time.time()
    
    # åŠ è½½é…ç½®
    config, source_dirs, target_dir, list_file, log_file, max_workers, retry_attempts = load_configuration()
    
    # æ¸…ç†ç›®æ ‡ç›®å½•
    cleanup_target_directory(target_dir)
    
    # æ„å»ºæ–‡ä»¶ç´¢å¼•
    index = build_file_index(source_dirs)
    
    # è¯»å–åŸå§‹æ–‡ä»¶åˆ—è¡¨
    original_files = read_original_file_list(list_file)
    total_files = len(original_files)
    
    # é¢„å¤„ç†æœç´¢å
    search_items = [(orig, clean_filename(orig)) for orig in original_files]
    
    # å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶å¤åˆ¶
    print("ğŸ“¦ å¼€å§‹å¹¶è¡Œå¤åˆ¶æ–‡ä»¶...")
    result_log = []
    found_count = 0
    not_found_count = 0
    copy_errors = 0
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = [executor.submit(process_item, item, target_dir, index, retry_attempts) 
                  for item in search_items]
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        for future in tqdm(as_completed(futures), total=len(futures), 
                          desc="â³ å¤åˆ¶ä¸­", unit="æ–‡ä»¶"):
            result = future.result()
            result_log.append(result)
            
            # ç»Ÿè®¡ç»“æœ
            if result["status"] == "success":
                found_count += 1
            elif result["status"] == "not_found":
                not_found_count += 1
            elif result["status"] == "error":
                copy_errors += 1
    
    # å†™å…¥ç»“æœæ—¥å¿—
    write_result_log(log_file, result_log)
    
    # è®¡ç®—æ€»è€—æ—¶
    total_time = time.time() - program_start_time
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 60)
    print(f"  æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"  âœ… æˆåŠŸå¤åˆ¶: {found_count} ({found_count/max(1, total_files):.1%})")
    print(f"  âŒ æœªæ‰¾åˆ°: {not_found_count} ({not_found_count/max(1, total_files):.1%})")
    print(f"  âš ï¸ å¤åˆ¶é”™è¯¯: {copy_errors}")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.1f}ç§’ | å¹³å‡é€Ÿåº¦: {total_files / max(1, total_time):.1f} æ–‡ä»¶/ç§’")
    print("=" * 60)
    
    # è­¦å‘Šæ£€æŸ¥
    failure_rate = (not_found_count + copy_errors) / max(1, total_files)
    if failure_rate > 0.5:
        print(f"\nâš ï¸ è­¦å‘Š: è¶…è¿‡50%çš„æ–‡ä»¶å¤„ç†å¤±è´¥ ({failure_rate:.1%})ï¼")
        print("å¯èƒ½çš„åŸå› :")
        print("  - ç½‘ç»œé©±åŠ¨å™¨è¿æ¥å¼‚å¸¸")
        print("  - æºç›®å½•è·¯å¾„ä¸æ­£ç¡®")
        print("  - æ–‡ä»¶å‘½åä¸åŒ¹é…")
        print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶å’Œç½‘ç»œè¿æ¥çŠ¶æ€")
    
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")

if __name__ == "__main__":
    main()